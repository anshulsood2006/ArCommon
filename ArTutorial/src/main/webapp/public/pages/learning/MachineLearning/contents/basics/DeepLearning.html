<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="keywords" content="computer programming,Deep Learning, Neural Network, Sigmoid, Perceptron">
<meta name="author" content="Anshul Sood">
<title>Deep Learning Basics</title>
</head>
<body>
<h2><a href="../../Content.html">Back</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../../Content.html">Home</a></h2>
<h1>Deep Learning</h1>
<h2>Neural Network</h2>
<ul>
	<li>In neural network, the computer learns how to solve a problem using observational data.</li>
	<li>The techniques used in neural network is called <b>deep learning.</b></li>
	<li>An example is the problem of teaching a computer to recognize handwritten digits.</li>
	<li>Two types of artificial neurons are
		<ul>
			<li><b>Perceptron neuron:</b>
				<ul>
					<li>It was developed by Frank Rosenblatt.</li>
					<li>It takes several binary inputs, x1,x2,..., and produces a single binary output.</li>
					<li>Mathematically for a perceptron:<br/><br/>
						<b>
							<table>
								<tr><td rowspan="2"> output &nbsp;&nbsp;&nbsp;&nbsp;   = </td><td>0 if &Sigma; <sub>j</sub> w<sub>j</sub>x<sub>j</sub> &lt;= Threshold</td></tr>
								<tr><td>1 if &Sigma; <sub>j</sub> w<sub>j</sub>x<sub>j</sub> &gt; Threshold</td></tr>
								<tr><td colspan="2">where<br/>
													w represents the weightage for the input factor and can be any real number. The larger value of w indicates that the factor matters a lot.<br/>
													x represents the input factor and is represented either as 0 or 1.<br/>
													j represents the number of input factors.<br/>
													Threshold is the real number and is chosen appropriately as per requirement.</td>
							</table>
						</b><br/><br/>
					</li>
					<li>The final output/decision making depends on the value of weights and threshold chosen.</li>
					<li>Multiple layers of perceptrons can be used to mimick human decision making as shown below:<br/>
						<img src="../../images/perceptron.jpg" alt="Decison Making using perceptron" height="300" width="50%"/>
						<ul>
							<li>The first layer of perceptrons is making three very simple decisions, by weighing the input evidences.</li>
							<li>The second layer of perceptrons is making a decision by weighing up the results from the first layer.</li>
						</ul>
					</li>
					<li>To simplify the above equation, a perceptron can also be written as:
						<b>
							<table>
								<tr><td rowspan="2"> output &nbsp;&nbsp;&nbsp;&nbsp;   = </td><td>0 if w * x + b &lt;= 0</td></tr>
								<tr><td>1 if w * x + b &gt; 0</td></tr>
								<tr><td colspan="2">where<br/>
													w * x represents the dot product of w and x.<br/>
													b = -Threshold is called <b>bias</b> which is a measure of how easy it is to get the perceptron to output a 1.</td>
							</table>
						</b><br/><br/>
					</li>
					<li>For a perceptron with two inputs A and B, each with weight -2 and an overall bias of 3 the output can be calculated as: <br/>
						(0 * -2) + (0 * -2) + 3 = 3 > 0 so output is 1 <br/>
						(0 * -2) + (1 * -2) + 3 = 1 > 0 so output is 1 <br/> 
						(1 * -2) + (0 * -2) + 3 = 1 > 0 so output is 1 <br/>
						(1 * -2) + (1 * -2) + 3 = -1 < 0 so output is 0 <br/>
						The truth table for above perceptron with various inputs is<br/>
						<table border="1">
							<tr><th>Input A</th><th>Input B</th><th>Output</th></tr>
							<tr><td>0</td><td>0</td><td>1</td></tr>
							<tr><td>0</td><td>1</td><td>1</td></tr>
							<tr><td>1</td><td>0</td><td>1</td></tr>
							<tr><td>1</td><td>1</td><td>0</td></tr>
						</table>
					</li>
					<li>The above truth table confirms that the perceptrons are equivalent to NAND gate. Since NAND is a universal gate, so we can infer that the perceptrons can be used to implement any boolean function.</li>
					<li>The sum bit on adding A and B is denoted as <b>A &#10753; B</b></li>
					<li>The carry bit on adding A and B is denoted as <b>AB</b></li>
					<li>If in a network of perceptrons the output from one perceptron is used twice as input to some other perceptron, in that case the double line can simply be merged into a single line with a weight of 2 * w (weight) instead of two line with each having w (weight).
						<ul>
							<li><br/><img src="../../images/doubleline.jpg" alt="Double line perceptron" height="120" width="25%"/></li>
							<li><br/><img src="../../images/singleline.jpg" alt="Single perceptron" height="120" width="25%"/></li>
						</ul>
					</li>
					<li>A input variables like x1 and x2 can also be notated as a <b>special perceptron</b> with no inputs but the output here would actually be the desired values x1. The above formula  <b> <table>
								<tr><td rowspan="2"> output &nbsp;&nbsp;&nbsp;&nbsp;   = </td><td>0 if &Sigma; <sub>j</sub> w<sub>j</sub>x<sub>j</sub> &lt;= Threshold</td></tr>
								<tr><td>1 if &Sigma; <sub>j</sub> w<sub>j</sub>x<sub>j</sub> &gt; Threshold</td></tr></table> does not</b> apply to this perceptron.
						<ul>
							<li><br/>
							<img src="../../images/noinput.jpg" alt="No input perceptron" height="30" width="10%"/>
							</li>
						</ul>
					</li>

				</ul>
			</li>
			<li><b>Sigmoid (Logistic) neuron: </b>
				<ul>
					<li>Using the network of perceptrons like above, we can devise learning algorithms which can automatically tune the weights and biases. For example, the inputs to the network might be the raw pixel data from a scanned, handwritten image of a digit. And we'd like the network to learn weights and biases so that the output from the network correctly classifies the digit.</li>
					<li>During learning:
						<ul>
							<li>The network will adjust the weights and biases so that the output of the network correctly identifies the digit.</li>
							<li>A small change in the weight <b>(w + &Delta; w)</b> during learning should cause only a small corresponding change in the output <b>output + &Delta; output</b> of the network .</li>
						</ul>
						But this can not happens when our network contains perceptrons, because sometimes even a small change in the weights or bias of any single perceptron in the network can cause the output of that perceptron to completely flip, say from 0 to 1 or otherwise.
					</li>
					<li>The above problem is solved using an artificial <b>sigmoid/logistic neuron</b>.</li>	
					<li>It takes several inputs x1,x2,... which can be any value between 0 and 1 (both inclusive) and produces <b> output = &sigma; (w * x + b)</b> where &sigma; is called <b>sigmoid/logistic function</b> and is defined as <br/>
						<b>&sigma;(z) = 1 / (1 + e <sup>-z</sup>) </b>
					</li>
					<li>So the output of sigmoid function viz. <b> output = &sigma; (w * x + b)</b> can be written as <b>output &sigma; (z) = 1 / (1 + e <sup>-z</sup>)</b> and subsequently as 
						<b>output &sigma; (w * x + b) = 1 / (1 + e <sup>-(&Sigma;<sub>j</sub> w<sub>j</sub> x<sub>j</sub> - b)</sup>)</b>
						<ul>
							<li>If <b>z = (w * x + b)  is very negative</b> then <b>e <sup>-z</sup> ~ &infin;</b> and hence <b>output &sigma; (z) ~ 0</b>.</li>
							<li>If <b>z = (w * x + b)  is very positive</b> then <b>e <sup>-z</sup> ~ 0</b> and hence <b>output &sigma; (z) ~ 1</b></li>
							<li>So for very positive and very negative values of z, sigmoid behaves just like a perceptron.</li>
							<li>For modest values of z, sigmoid behaves differently from a perceptron.</li>
							<li>As per basic calculas <br/>
								<b>&Delta; w = &Sigma; <sub>j</sub> (&delta;output/&delta;w<sub>j</sub>) &Delta;w<sub>j</sub> + (&delta;output/&delta;b) &Delta;b</b><br/>
								where <b>(&delta;output/&delta;w<sub>j</sub>)</b> and <b>(&delta;output/&delta;b)</b> denote partial derivatives of the output with respect to w<sub>j</sub> and b, respectively
							</li>
						</ul>
					</li>			
				</ul>
			</li>
		</ul>
	</li>
	<li>The standard learning algorithm for neural networks is <b>stochastic gradient descent</b>.</li>
	<li>The left most layer in a neural network is called <b>input layer</b></li>
	<li>The right most layer in a neural network is called <b>output layer</b></li>
	<li>The group of inner layers in a neural network is called <b>hidden layer</b></li>
	<li>All the layers together are called <b>multilayer perceptrons (MLPs)</b> (even if they are made of sigmoid neurons).</li>
	<li><b>Feed forward neural networks:</b>
		<ul>
			<li>The neural networks where the output from one layer is used as input to the next layer.</li>
			<li>There are no loops in the network.</li>
			<li>The information is always fed forward, never fed back.</li>
		</ul>
	</li>
	<li><b>Recurrent neural networks:</b>
		<ul>
			<li>There are loops in the network.</li>
			<li>There are some neurons in the network which fires (that means output &asymp; 1) for some limited duration of time, before becoming quiescent. That firing can stimulate other neurons, which may fire a little while later, also for a limited duration. That causes still more neurons to fire, and so over time we get a cascade of neurons firing.</li>
			<li>They are much closer in spirit to how our brains work than feedforward networks</li>
		</ul>
	</li>
	<li>The learning algorithms for recurrent networks are less powerful as compared to the algorithms for feed forward network.</li>
	<li><b>Activation Function</b>
		<ul>
			<li>It is the output of a node/neuron given an input or set of inputs. </li>
			<li><b>Activation fires</b> means <b>output of the node/neuron &asymp; 1</b></li>
		</ul>
	</li>
</ul>
<h2>Neural network to recognize handwritten digits</h2>
<h3>Design of the network</h3>
Our network will consist of three layers:
<ul>
	<li><b>Input layer </b>is consisting of the neurons which encode the values of input pixels from the image. If the size of image is 28 * 28 pixels then the number of input neurons will be 28 * 28 = 784. These neurons will have a value ranging between 0 for white and 1 for black. The intermediate values represent darkening shades of grey.</li>
	<li><b>Hidden layer </b>
		<ul>
			<li>This layer is consisting of n number of neurons where n is chosen experimentally. </li>
			<li>The neurons of this layer weights the inputs from input layer.</li>
			<li>Large value of weight (w) is given to the input neurons which overlap with the image.</li>
			<li>The other input neurons are given small weightage.</li>
		</ul>
	</li>
	<li><b>Output layer </b>is consisting of 10 neurons. The neuron which fires (i.e. gives an output of 1) will indicate the digit in the inputs image. e.g. if the output of 1st neuron is 1 then the digit recognized will be 0 and similarly if the output of 6th neuron is 1 then the digit recognized will be 7 </li>
	<li><br/>
		<img src="../../images/design.jpg" alt="Design" height="400" width="40%"/>
	</li>
</ul>
<h3>Implementation of the network</h3>
<ul>
	<li>We need to have training data to train our neural network.</li>
	<li>Then we need to have test data different from the above data to test the network. This data is totally different from the training data and thus it will give us more confidence that the the system designed is able to recognize any digit irrespective of whether that has been used during training it or not.</li>
	<li>Each training input is denoted as x and will be a 28 * 28 dimensional vector.</li>
	<li>Each entry in the vector represents grey value weight for the pixel</li>
	<li>The output of training data x is denoted as y = y(x) where y is a 10 dimensional vector.</li>
	<li>If training data x gives a output 6, this can be represented as y(x) = (0,0,0,0,0,0,1,0,0,0)<sup>T</sup> where T is the transpose operation, turning a row vector into an ordinary (column) vector</li>
	<li>We need to define a cost function as:<br/>
	<b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C(w,b) = 1/2n &Sigma; ||y(x) - a||<sup>2</sup></b> where <br/>
		<b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;w is the collection of all weights in the network.<br/>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b is all the biases in the network.<br/>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n is the total number of training inputs.<br/>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a is the vector of output from network<br/>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x is the training input.<br/>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;||v||denotes the usual length function for a vector v<br/></b></li>
	<li>C(w,b) is also called quadratic cost function or mean squared error (MSE)</li>
	<li>The value of a depends on the values of w, b and x.</li>
	<li>C(w,b) is always non negative</li>
	<li>C(w,b) &asymp; 0 when y(x) is approximately equal to the output a for an input x</li>
	<li>The training algorithm will be accurate if the cost function C(w,b) &asymp; 0</li>
	<li>That means aim of the training algorithm is to minimize the cost C(w,b) as a function of the weights and biases.</li>
	<li>The algorithm to do so is called <b>gradient descent</b> which minimizes a function of many variables.</li>
	<li>In real time, a neural network will have cost functions depending on billions of weights and biases which can not minimized using calculas.</li>
	<li>We can have an example of a ball where we need to roll it down to the bottom of the valley
		<ul>
			<li>
				For a ball to move to a minimum point on the valley, we move the ball a small amount &Delta;v<sub>1</sub> in the v<sub>1</sub> direction, and a small amount &Delta;v<sub>2</sub> in the v<sub>2</sub> direction. Calculus tells us that cost function C changes as follows:<br/>
				<b>&Delta;C &asymp;  (&delta;C/&delta;v<sub>1</sub>) &Delta;v<sub>1</sub> + (&delta;C/&delta;v<sub>2</sub>)&Delta;v<sub>2</sub></b>
			</li>
			<li>Since we need to roll down the ball to the bottom of valley, so we need to choose &Delta;v<sub>1</sub> and &Delta;v<sub>2</sub> in such a way that it makes &Delta;C negative
				<ul> Two equations we need here are
					<li>Vector of Change of i.e. <b>&Delta;v &equiv; (&Delta;v<sub>1</sub>, &Delta;v<sub>2</sub>)<sup>T</sup></b></li>
					<li>Gradient Vector of Cost Function i.e. <b>&nabla; C &equiv;<b></b></li>
				</ul>
			</li>
		</ul>
	</li>	
</ul>
<h1>Examples</h1>
<p><b></b></p>
<pre>
</pre>
<h1>Frequently Asked Questions</h1>
<p><b></b></p>
<p></p>
<p><b></b></p>
<p></p>
<h2><a href="../../Content.html">Back</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../../Content.html">Home</a></h2>
</body>
</html>