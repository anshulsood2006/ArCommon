<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="keywords" content="computer programming,Deep Learning, Neural Network">
<meta name="author" content="Anshul Sood">
<title>Deep Learning Basics</title>
</head>
<body>
<h2><a href="../../Content.html">Back</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../../Content.html">Home</a></h2>
<h1>Deep Learning</h1>
<h2>Neural Network</h2>
<ul>
	<li>In neural network, the computer learns how to solve a problem using observational data.</li>
	<li>The techniques used in neural network is called <b>deep learning.</b></li>
	<li>An example is the problem of teaching a computer to recognize handwritten digits.</li>
	<li>Two types of artificial neurons are
		<ul>
			<li><b>Perceptron neuron:</b>
				<ul>
					<li>It was developed by Frank Rosenblatt.</li>
					<li>It takes several binary inputs, x1,x2,..., and produces a single binary output.</li>
					<li>Mathematically for a perceptron:<br/><br/>
						<b>
							<table>
								<tr><td rowspan="2"> output &nbsp;&nbsp;&nbsp;&nbsp;   = </td><td>0 if &Sigma; <sub>j</sub> w<sub>j</sub>x<sub>j</sub> &lt;= Threshold</td></tr>
								<tr><td>1 if &Sigma; <sub>j</sub> w<sub>j</sub>x<sub>j</sub> &gt; Threshold</td></tr>
								<tr><td colspan="2">where<br/>
													w represents the weightage for the input factor and can be any real number. The larger value of w indicates that the factor matters a lot.<br/>
													x represents the input factor and is represented either as 0 or 1.<br/>
													j represents the number of input factors.<br/>
													Threshold is the real number and is chosen appropriately as per requirement.</td>
							</table>
						</b><br/><br/>
					</li>
					<li>The final output/decision making depends on the value of weights and threshold chosen.</li>
					<li>Multiple layers of perceptrons can be used to mimick human decision making as shown below:<br/>
						<img src="../../images/perceptron.jpg" alt="Decison Making using perceptron" height="300" width="50%"/>
						<ul>
							<li>The first layer of perceptrons is making three very simple decisions, by weighing the input evidences.</li>
							<li>The second layer of perceptrons is making a decision by weighing up the results from the first layer.</li>
						</ul>
					</li>
					<li>To simplify the above equation, a perceptron can also be written as:
						<b>
							<table>
								<tr><td rowspan="2"> output &nbsp;&nbsp;&nbsp;&nbsp;   = </td><td>0 if w * x + b &lt;= 0</td></tr>
								<tr><td>1 if w * x + b &gt; 0</td></tr>
								<tr><td colspan="2">where<br/>
													w * x represents the dot product of w and x.<br/>
													b = -Threshold is called <b>bias</b> which is a measure of how easy it is to get the perceptron to output a 1.</td>
							</table>
						</b><br/><br/>
					</li>
					<li>For a perceptron with two inputs A and B, each with weight -2 and an overall bias of 3 the output can be calculated as: <br/>
						(0 * -2) + (0 * -2) + 3 = 3 > 0 so output is 1 <br/>
						(0 * -2) + (1 * -2) + 3 = 1 > 0 so output is 1 <br/> 
						(1 * -2) + (0 * -2) + 3 = 1 > 0 so output is 1 <br/>
						(1 * -2) + (1 * -2) + 3 = -1 < 0 so output is 0 <br/>
						The truth table for above perceptron with various inputs is<br/>
						<table border="1">
							<tr><th>Input A</th><th>Input B</th><th>Output</th></tr>
							<tr><td>0</td><td>0</td><td>1</td></tr>
							<tr><td>0</td><td>1</td><td>1</td></tr>
							<tr><td>1</td><td>0</td><td>1</td></tr>
							<tr><td>1</td><td>1</td><td>0</td></tr>
						</table>
					</li>
					<li>The above truth table confirms that the perceptrons are equivalent to NAND gate. Since NAND is a universal gate, so we can infer that the perceptrons can be used to implement any boolean function.</li>
					<li>The sum bit on adding A and B is denoted as <b>A &#10753; B</b></li>
					<li>The carry bit on adding A and B is denoted as <b>AB</b></li>
					<li>If in a network of perceptrons the output from one perceptron is used twice as input to some other perceptron, in that case the double line can simply be merged into a single line with a weight of 2 * w (weight) instead of two line with each having w (weight).
						<ul>
							<li><br/><img src="../../images/doubleline.jpg" alt="Double line perceptron" height="120" width="25%"/></li>
							<li><br/><img src="../../images/singleline.jpg" alt="Single perceptron" height="120" width="25%"/></li>
						</ul>
					</li>
					<li>A input variables like x1 and x2 can also be notated as a <b>special perceptron</b> with no inputs but the output here would actually be the desired values x1. The above formula  <b> <table>
								<tr><td rowspan="2"> output &nbsp;&nbsp;&nbsp;&nbsp;   = </td><td>0 if &Sigma; <sub>j</sub> w<sub>j</sub>x<sub>j</sub> &lt;= Threshold</td></tr>
								<tr><td>1 if &Sigma; <sub>j</sub> w<sub>j</sub>x<sub>j</sub> &gt; Threshold</td></tr></table> does not</b> apply to this perceptron.
						<ul>
							<li><br/>
							<img src="../../images/noinput.jpg" alt="No input perceptron" height="30" width="10%"/>
							</li>
						</ul>
					</li>

				</ul>
			</li>
			<li><b>Sigmoid (Logistic) neuron: </b>
				<ul>
					<li>Using the network of perceptrons like above, we can devise learning algorithms which can automatically tune the weights and biases. For example, the inputs to the network might be the raw pixel data from a scanned, handwritten image of a digit. And we'd like the network to learn weights and biases so that the output from the network correctly classifies the digit.</li>
					<li>During learning:
						<ul>
							<li>The network will adjust the weights and biases so that the output of the network correctly identifies the digit.</li>
							<li>A small change in the weight <b>(w + &Delta; w)</b> during learning should cause only a small corresponding change in the output <b>output + &Delta; output</b> of the network .</li>
						</ul>
						But this can not happens when our network contains perceptrons, because sometimes even a small change in the weights or bias of any single perceptron in the network can cause the output of that perceptron to completely flip, say from 0 to 1 or otherwise.
					</li>
					<li>The above problem is solved using an artificial <b>sigmoid/logistic neuron</b>.</li>	
					<li>It takes several inputs x1,x2,... which can be any value between 0 and 1 (both inclusive) and produces <b> output = &sigma; (w * x + b)</b> where &sigma; is called <b>sigmoid/logistic function</b> and is defined as <br/>
						<b>&sigma;(z) = 1 / (1 + e <sup>-z</sup>) </b>
					</li>
					<li>So the output of sigmoid function viz. <b> output = &sigma; (w * x + b)</b> can be written as <b>output &sigma; (z) = 1 / (1 + e <sup>-z</sup>)</b> and subsequently as 
						<b>output &sigma; (w * x + b) = 1 / (1 + e <sup>-(&Sigma;<sub>j</sub> w<sub>j</sub> x<sub>j</sub> - b)</sup>)</b>
						<ul>
							<li>If <b>z = (w * x + b)  is very negative</b> then <b>e <sup>-z</sup> ~ &infin;</b> and hence <b>output &sigma; (z) ~ 0</b>.</li>
							<li>If <b>z = (w * x + b)  is very positive</b> then <b>e <sup>-z</sup> ~ 0</b> and hence <b>output &sigma; (z) ~ 1</b></li>
							<li>So for very positive and very negative values of z, sigmoid behaves just like a perceptron.</li>
							<li>For modest values of z, sigmoid behaves differently from a perceptron.</li>
							<li>As per basic calculas <br/>
								<b>&Delta; w = &Sigma; <sub>j</sub> (&delta;output/&delta;w<sub>j</sub>) &Delta;w<sub>j</sub> + (&delta;output/&delta;b) &Delta;b</b><br/>
								where <b>(&delta;output/&delta;w<sub>j</sub>)</b> and <b>(&delta;output/&delta;b)</b> denote partial derivatives of the output with respect to w<sub>j</sub> and b, respectively
							</li>
						</ul>
					</li>			
				</ul>
			</li>
		</ul>
	</li>
	<li>The standard learning algorithm for neural networks is <b>stochastic gradient descent</b>.</li>
</ul>
<h1>Examples</h1>
<p><b></b></p>
<pre>
</pre>
<h1>Frequently Asked Questions</h1>
<p><b></b></p>
<p></p>
<p><b></b></p>
<p></p>
<h2><a href="../../Content.html">Back</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../../Content.html">Home</a></h2>
</body>
</html>